# Story 4.1.6: Performance Test Schema

## Status: To Do

## Story

- **As a** Developer
- **I want** to seed the event store with 1M+ test events and run performance tests against the new schema
- **so that** I can ensure query performance meets our latency targets under load.

## Acceptance Criteria

1. A script or test utility is created to populate the `domain_events` table with at least 1 million realistic test events across multiple tenants and aggregates.
2. Performance tests are created to measure the query latency for common `EafPostgresEventStorageEngine` operations (e.g., reading event streams for a `TrackingEventProcessor`).
3. The tests verify that query performance for streaming events remains within the defined SLA (e.g., <50ms per batch).
4. The database query plans (`EXPLAIN ANALYZE`) are reviewed to confirm that the correct indexes are being used.

## Tasks / Subtasks

- [ ] **Task 1: Create Event Data Generation Utility** (AC: 1)
  - [ ] Create `EventStoreDataGenerator.kt` in test utilities package
  - [ ] Generate realistic multi-tenant event data (10+ tenants)
  - [ ] Create diverse aggregate types (User, Order, Product, etc.) with realistic event patterns
  - [ ] Generate events with realistic payload sizes (100B - 10KB range)
  - [ ] Implement batch insertion for optimal performance (10K events per batch)
  - [ ] Add progress tracking and memory management for large datasets

- [ ] **Task 2: Implement Multi-Aggregate Event Patterns**
  - [ ] Create realistic aggregate lifecycle patterns (create → modify → finalize)
  - [ ] Generate event streams with varying lengths (1-1000 events per aggregate)
  - [ ] Implement temporal patterns (events spread over realistic time periods)
  - [ ] Create realistic metadata and correlation data
  - [ ] Include snapshot events at appropriate intervals (every 100 events)

- [ ] **Task 3: Performance Test Framework Setup** (AC: 2)
  - [ ] Create `EventStorePerformanceTest.kt` using JMH (Java Microbenchmark Harness)
  - [ ] Set up Testcontainers PostgreSQL with production-like configuration
  - [ ] Configure database connection pooling for realistic load simulation
  - [ ] Create test scenarios for all `EafPostgresEventStorageEngine` operations
  - [ ] Implement concurrent load simulation for multi-user scenarios

- [ ] **Task 4: Core Operation Performance Tests** (AC: 2, 3)
  - [ ] Benchmark `appendEvents()` with various batch sizes (1, 10, 100 events)
  - [ ] Test `readEvents(streamId, fromSequence)` for aggregate loading scenarios
  - [ ] Benchmark `readEvents(trackingToken, mayBlock)` for event streaming
  - [ ] Test snapshot storage and retrieval performance
  - [ ] Measure `createHeadToken()` and `createTailToken()` performance

- [ ] **Task 5: Event Streaming Performance Analysis** (AC: 3)
  - [ ] Test TrackingEventProcessor event consumption rates
  - [ ] Measure latency for streaming 1000-event batches
  - [ ] Test concurrent streaming scenarios (multiple processors)
  - [ ] Benchmark catch-up scenarios (processing large event backlogs)
  - [ ] Verify performance with realistic TrackingToken progression

- [ ] **Task 6: Database Query Optimization Verification** (AC: 4)
  - [ ] Capture and analyze EXPLAIN ANALYZE results for all test queries
  - [ ] Verify index usage for tenant isolation queries
  - [ ] Confirm composite index effectiveness for streaming queries
  - [ ] Analyze query plans under concurrent load
  - [ ] Document query optimization recommendations

- [ ] **Task 7: Load Testing and Stress Testing**
  - [ ] Test sustained event append rates (1000+ events/second)
  - [ ] Simulate multiple concurrent TrackingEventProcessors
  - [ ] Test memory usage during large dataset processing
  - [ ] Measure garbage collection impact under load
  - [ ] Test database connection pool behavior under stress

- [ ] **Task 8: Performance Regression Testing**
  - [ ] Create baseline performance measurements
  - [ ] Establish performance SLA thresholds and alerts
  - [ ] Create automated performance regression detection
  - [ ] Document performance characteristics across different data volumes
  - [ ] Create performance monitoring dashboards

## Dev Technical Guidance

- **Test Location**: `libs/eaf-eventsourcing-sdk/src/test/kotlin/com/axians/eaf/eventsourcing/performance/`
- **Dependencies Required**: Add to `libs/eaf-eventsourcing-sdk/build.gradle.kts`:

  ```kotlin
  testImplementation(libs.jmh.core)
  testImplementation(libs.jmh.generator.annprocess)
  testImplementation(libs.testcontainers.postgresql)
  testImplementation(libs.hikaricp)
  testImplementation(libs.kotlinx.coroutines.test)
  ```

- **Performance Targets**:
  - Event append: <10ms for single event, <50ms for 100-event batch
  - Aggregate loading: <20ms for 1000-event aggregate
  - Event streaming: <50ms for 1000-event batch
  - Token operations: <1ms for all token creation/comparison operations
- **Test Data Characteristics**:
  - 10+ tenants with realistic distribution (80/20 rule)
  - 1000+ aggregates per tenant with realistic lifecycle patterns
  - Event payload sizes: 70% small (100-500B), 25% medium (1-5KB), 5% large (5-10KB)
  - Temporal distribution: realistic business hour patterns over months
- **Database Configuration**: Use production-like settings:
  - Connection pool: 20-50 connections
  - PostgreSQL: Properly tuned for write-heavy workloads
  - Memory: adequate shared_buffers and work_mem settings
- **Measurement Strategy**:
  - Use JMH for microsecond-level precision
  - Include warm-up iterations to avoid JIT compilation bias
  - Measure 95th percentile latencies, not just averages
  - Monitor system resources (CPU, memory, disk I/O) during tests

## Testing Guidance

- **Objective**: Validate that the enhanced schema performs at production scale with realistic loads
- **Key Test Scenarios**:
  - **Baseline Performance Tests**:
    - Single-threaded operation benchmarks
    - Memory usage patterns during large operations
    - Database resource utilization measurement
    - Query execution plan analysis
  - **Concurrent Load Tests**:
    - Multiple TrackingEventProcessors consuming events simultaneously
    - Concurrent aggregate loading and event appending
    - Mixed read/write workloads simulating production usage
    - Database lock contention and deadlock detection
  - **Scale Tests**:
    - Performance degradation analysis as dataset grows (1M → 10M → 100M events)
    - Index effectiveness at different data volumes
    - Memory requirements for large query result sets
    - Garbage collection behavior under sustained load
  - **Tenant Isolation Performance**:
    - Verify no tenant queries affect other tenant performance
    - Test tenant-specific index effectiveness
    - Measure overhead of tenant filtering in all queries
- **Performance Regression Detection**:
  - Automated comparison against baseline measurements
  - Performance alerts for SLA violations
  - Trend analysis for gradual performance degradation
  - Integration with CI/CD pipeline for performance gates
- **Success Criteria**:
  - All operations meet established SLA targets under load
  - Database indexes are utilized effectively (confirmed by EXPLAIN ANALYZE)
  - Memory usage remains stable during extended test runs
  - No significant performance degradation at 10M+ event volumes
  - Concurrent operations scale linearly up to hardware limits
- **Failure Analysis**:
  - Performance bottleneck identification and mitigation strategies
  - Database tuning recommendations for production deployment
  - Index optimization opportunities
  - Application-level caching strategies if needed
- **Tools**: JMH, PostgreSQL EXPLAIN ANALYZE, Testcontainers, VisualVM/JProfiler, database monitoring tools (pg_stat_statements)
