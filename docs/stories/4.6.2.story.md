# Story 4.6.2: Conduct and Analyze Load Tests

## Status: To Do

## Story
- **As a** Performance Engineer
- **I want** to run comprehensive load tests against the new event-driven system
- **so that** I can validate it meets our performance targets under production load.

## Acceptance Criteria
1. Load test scenarios are created to simulate realistic production traffic patterns.
2. Tests are executed to verify the system can handle the target throughput (> 400 RPS) with acceptable latency (< 25ms).
3. The performance of both command processing and event propagation is measured.
4. Bottlenecks and optimization opportunities are identified and documented.
5. The test results are analyzed and compared against the baseline performance of the legacy system.

## Tasks / Subtasks

- [ ] **Task 1: Design Realistic Load Test Scenarios** (AC: 1)
  - [ ] Analyze production traffic patterns and user behavior
  - [ ] Create user journey scenarios (user creation, activation, role changes)
  - [ ] Design multi-tenant load patterns with realistic distribution
  - [ ] Create peak load and sustained load test scenarios
  - [ ] Add error injection and edge case scenarios

- [ ] **Task 2: Set up Load Testing Infrastructure** (AC: 2)
  - [ ] Configure load testing environment with production-like setup
  - [ ] Set up performance testing tools (K6, Artillery, or JMeter)
  - [ ] Create test data generation and management
  - [ ] Configure monitoring and metrics collection during tests
  - [ ] Set up baseline measurement infrastructure

- [ ] **Task 3: Implement Command Processing Load Tests** (AC: 3)
  - [ ] Create command-focused load test scenarios
  - [ ] Test user aggregate command processing (CreateUser, ActivateUser, etc.)
  - [ ] Measure command validation and business logic performance
  - [ ] Test concurrent command processing with optimistic locking
  - [ ] Validate command gateway throughput and latency

- [ ] **Task 4: Event Processing and Propagation Tests** (AC: 3)
  - [ ] Test event processing pipeline performance
  - [ ] Measure event projection lag under load
  - [ ] Test NATS event publishing throughput and reliability
  - [ ] Validate audit trail projection performance
  - [ ] Test tracking event processor performance

- [ ] **Task 5: End-to-End Performance Testing** (AC: 2)
  - [ ] Create comprehensive end-to-end user scenarios
  - [ ] Test complete request-to-event-to-projection flow
  - [ ] Measure total system latency and throughput
  - [ ] Test system behavior under sustained load (>400 RPS)
  - [ ] Validate latency targets (<25ms for 95th percentile)

- [ ] **Task 6: Bottleneck Identification and Analysis** (AC: 4)
  - [ ] Analyze performance test results and metrics
  - [ ] Identify system bottlenecks and resource constraints
  - [ ] Profile database query performance and optimization
  - [ ] Analyze memory usage and garbage collection impact
  - [ ] Document optimization opportunities and recommendations

- [ ] **Task 7: Legacy System Comparison** (AC: 5)
  - [ ] Establish baseline performance metrics for legacy system
  - [ ] Run comparable load tests against legacy implementation
  - [ ] Compare throughput, latency, and resource utilization
  - [ ] Document performance improvements and regressions
  - [ ] Create performance comparison report and recommendations

- [ ] **Task 8: Performance Optimization and Tuning**
  - [ ] Implement identified performance optimizations
  - [ ] Tune database connection pools and query performance
  - [ ] Optimize JVM settings and garbage collection
  - [ ] Configure Axon framework performance settings
  - [ ] Re-run tests to validate improvements

## Dev Technical Guidance

- **Test Location**: `apps/acci-eaf-control-plane/src/test/performance/`
- **Dependencies Required**: Add to build.gradle.kts:
  ```kotlin
  testImplementation(libs.k6.gradle.plugin)
  testImplementation(libs.gatling.charts.highcharts)
  testImplementation(libs.spring.boot.starter.test)
  testImplementation(libs.testcontainers.postgresql)
  ```
- **Load Test Configuration Example (K6)**:
  ```javascript
  import http from 'k6/http';
  import { check } from 'k6';
  
  export let options = {
    scenarios: {
      constant_load: {
        executor: 'constant-vus',
        vus: 50,
        duration: '10m',
      },
      spike_test: {
        executor: 'ramping-vus',
        startVUs: 0,
        stages: [
          { duration: '2m', target: 100 },
          { duration: '5m', target: 100 },
          { duration: '2m', target: 200 },
          { duration: '5m', target: 200 },
          { duration: '2m', target: 0 },
        ],
      },
    },
    thresholds: {
      http_req_duration: ['p(95)<25'], // 95% of requests must be below 25ms
      http_req_failed: ['rate<0.01'],   // Error rate must be below 1%
    },
  };
  
  export default function() {
    let response = http.post('http://localhost:8080/api/users', {
      email: 'user@example.com',
      username: 'testuser'
    });
    
    check(response, {
      'status is 201': (r) => r.status === 201,
      'response time < 25ms': (r) => r.timings.duration < 25,
    });
  }
  ```
- **Performance Targets**:
  - **Throughput**: >400 requests/second sustained
  - **Latency**: <25ms for 95th percentile
  - **Error Rate**: <1% under normal load
  - **Event Processing Lag**: <1 second under peak load
  - **Resource Utilization**: <80% CPU, <70% memory
- **Test Environment Setup**:
  ```yaml
  # docker-compose.performance.yml
  version: '3.8'
  services:
    app:
      build: .
      environment:
        - SPRING_PROFILES_ACTIVE=performance
        - JVM_OPTS=-Xmx2g -Xms2g -XX:+UseG1GC
      ports:
        - "8080:8080"
    
    postgres:
      image: postgres:15-alpine
      environment:
        - POSTGRES_DB=eaf_performance
        - shared_buffers=256MB
        - max_connections=200
    
    prometheus:
      image: prom/prometheus
      volumes:
        - ./prometheus.yml:/etc/prometheus/prometheus.yml
  ```
- **Monitoring During Tests**: Collect JVM metrics, database performance, event processing lag, NATS throughput
- **Test Data Management**: Use realistic data volumes and distributions matching production patterns

## Testing Guidance

- **Objective**: Validate system performance meets production requirements under realistic load conditions
- **Key Test Scenarios**:
  - **Baseline Performance Tests**:
    - Establish single-user performance baseline
    - Test individual component performance (database, NATS, etc.)
    - Measure cold start and warm-up performance
    - Validate configuration optimization effectiveness
  - **Load Ramp-up Tests**:
    - Gradually increase load from 0 to target RPS
    - Identify performance degradation points
    - Test system stability under increasing load
    - Validate automatic scaling if configured
  - **Sustained Load Tests**:
    - Run target load (400+ RPS) for extended periods (>30 minutes)
    - Monitor for memory leaks and resource degradation
    - Test system stability and recovery
    - Validate performance consistency over time
  - **Spike and Burst Tests**:
    - Test sudden load increases (2x, 5x normal load)
    - Validate system recovery after load spikes
    - Test rate limiting and circuit breaker behavior
    - Measure impact on ongoing operations
  - **Multi-Tenant Load Tests**:
    - Simulate realistic tenant distribution (80/20 rule)
    - Test tenant isolation under load
    - Validate performance consistency across tenants
    - Test tenant-specific rate limiting
  - **Failure Scenario Tests**:
    - Test performance during database failover
    - Validate behavior during NATS connectivity issues
    - Test recovery performance after outages
    - Measure degraded mode performance
- **Performance Test Implementation**:
  ```kotlin
  @Test
  fun `should handle 400 RPS with acceptable latency`() {
      val loadTest = LoadTestBuilder()
          .withTargetRPS(400)
          .withDuration(Duration.ofMinutes(10))
          .withScenario(userCreationScenario())
          .build()
      
      val results = loadTest.execute()
      
      assertThat(results.averageRPS).isGreaterThan(400)
      assertThat(results.p95Latency).isLessThan(Duration.ofMillis(25))
      assertThat(results.errorRate).isLessThan(0.01)
  }
  ```
- **Success Criteria**: 
  - Sustained throughput >400 RPS for 30+ minutes
  - 95th percentile latency <25ms
  - Error rate <1% under target load
  - Event processing lag <1 second
  - System remains stable and responsive
- **Performance Baseline Comparison**:
  - Document current legacy system performance
  - Measure improvement/regression in each metric
  - Identify areas requiring optimization
  - Validate performance improvement ROI
- **Bottleneck Analysis Process**:
  - Use profiling tools (JProfiler, VisualVM) during load tests
  - Analyze database query performance and optimization
  - Review event processing pipeline efficiency
  - Examine network and I/O bottlenecks
- **Load Test Data Management**:
  - Use production-like data volumes and complexity
  - Implement data cleanup between test runs
  - Ensure test data doesn't affect results consistency
  - Use realistic user behavior patterns
- **Environment Validation**:
  - Ensure test environment matches production specifications
  - Validate monitoring and alerting during tests
  - Test load balancer and infrastructure scaling
  - Verify resource allocation and constraints
- **Tools**: K6/Artillery/JMeter for load generation, Application Performance Monitoring (APM), database performance monitoring, JVM profiling tools, infrastructure monitoring
